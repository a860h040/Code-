Slide 29 — Interactive Workshop (10 minutes)

Breakout or group chat activity:
“Design the perfect forecasting dashboard for APPE students to use.”

Ask them to choose:

3 KPIs

2 alerts

1 scenario button

1 ‘variance story’ output

You collect answers live.

Slide 30 — Suggested “Best” KPI Set (Answer slide)

KPIs (starter set):

Total forecast accuracy (monthly absolute % error)

Bias (over vs under)

Explainable variance rate

Top 20 drug forecast accuracy

Time to identify driver (days)

Slide 31 — Mini Case #1 (Interactive)

Case: “Shortage hits your top antibiotic; spend jumps $400k.”
Ask:

What do you change in forecast? (scenario)

What do you report as driver? (shortage/substitution)

What metric shows this was explainable? (variance attribution)

Slide 32 — Mini Case #2 (Interactive)

Case: “Biosimilar adoption is slower than expected.”
Ask:

Which parameter would you adjust? (adoption curve)

What data would you use to estimate ramp? (historic conversion rate, provider adoption, policy timeline)

Slide 33 — Mini Case #3 (Interactive)

Case: “Site-of-care shift moves infusion spend out of hospital outpatient.”
Ask:

Is total spend changed or just location?

How should the forecast report handle it? (separate by site)

Slide 34 — Common Pitfalls (Quick)

Forecasting only total spend (misses drug-level drivers)

Ignoring mix/site-of-care

No standard definitions of “accuracy”

No routine monthly variance review cadence

Slide 35 — Implementation Checklist (Deliverable Slide)

Forecasting cadence (monthly):

Refresh actuals

Review top variances

Attribute drivers

Update scenarios

Communicate “variance story”

Track accuracy + explainability

Slide 36 — Quick Kahoot / Quiz (5 questions)

Spend = ___ × ___

“Good accuracy” should include % error AND ___

Name 2 volatility drivers

Why is benchmarking hard? (definitions + peer grouping)

Name one DBF enhancement that would help APPE users

Slide 37 — Key Takeaways (Wrap)

Forecasting is manageable with a repeatable workflow

Define “good” as accuracy + explainability

Benchmarking can be powerful if definitions and peer groups are aligned

Vizient tools can be strengthened with scenario modeling + early warning + driver attribution

Slide 38 — Q&A

“Ask anything—tools, metrics, or how you’d implement at a hospital.”
